from typing import List

from langchain.schema import Document
from langchain_community.document_loaders import WebBaseLoader
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.vectorstores import InMemoryVectorStore
from langchain_groq import ChatGroq
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter
from prompts.rag_system_prompt import rag_system_prompt


class RagBot():
    """
    Class to create a Retrieval-Augmented Generation (RAG) agent using a language model (LLM),
    a system prompt, embeddings, and a set of documents/pages.

    This class integrates information retrieval and response generation functionalities
    based on a defined processing pipeline.

    Attributes:
        llm (ChatGroq): The language model used for response generation.
        prompt (ChatPromptTemplate): System prompt template that defines the agent's context.
        embedding (HuggingFaceEmbeddings): Embedding model used for encoding documents.
        output_parser (StrOutputParser): Parser to process the LLM's output.
        runnable (RunnablePassthrough): Component used in the pipeline for data forwarding.
        docs: A set of documents or pages used as the knowledge base for RAG.
    """
    def __init__(
            self, 
            llm: ChatGroq, 
            prompt: ChatPromptTemplate, 
            embedding: HuggingFaceEmbeddings, 
            documents : List[Document]
    ) -> None:
        """
        Initializes the RAG agent with the required components.

        Args:
            llm (ChatGroq): The language model used for response generation.
            prompt (ChatPromptTemplate): Template for configuring the system's behavior.
            embedding (HuggingFaceEmbeddings): Model used for generating document embeddings.
            documents: A set of documents to be used as the knowledge base.
        """
        self.llm       = llm 
        self.prompt    = prompt
        self.embedding = embedding
        self.documents = documents 

    def loader(self) -> List[Document]:
        """
        Loads documents from the web using specified URLs.

        This method uses the `WebBaseLoader` to access the provided URLs and returns
        the loaded documents as `Document` objects. These documents may contain
        the content and relevant metadata of the pages.

        Returns:
            List[Document]: A list of `Document` objects representing the loaded
            content from the specified URLs.
        """
        loader = WebBaseLoader(self.documents)
        return loader.load()
    
    def splitter(self) -> List[Document]:
        """
        Splits long documents into smaller chunks for easier processing.

        This method utilizes the `RecursiveCharacterTextSplitter` to segment
        the loaded documents into smaller chunks, adhering to the defined size
        and overlap rules. The goal is to create text fragments that are easier
        to analyze or process by language models.

        Returns:
            List[Document]: A list of `Document` objects where each document
            represents a fragment of the original content.
        """
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size         = 500, 
            chunk_overlap      = 50, 
            length_function    = len,
            separators         = ["", " ", ".", "\n", "\n\n"],
            is_separator_regex = False
        )
        return text_splitter.split_documents(self.loader())
        
    def retriever(self) -> InMemoryVectorStore:
        """
        Creates a retrieval mechanism based on embeddings to fetch relevant information
        from the provided documents.

        Returns:
            InMemoryVectorStore: A memory-based vector store that acts as both
            a storage and retrieval system for information encoded from the documents.
        """
        vectorstore = InMemoryVectorStore.from_documents(
            self.splitter(), 
            self.embedding
        )
        return vectorstore.as_retriever()
 
    def run(self, query: str) -> str: 
        """
        Executes a query through the RAG pipeline and returns the generated response
        from the language model.

        Args:
            query (str): User's question or input for which the agent should generate a response.

        Returns:
            str: Response generated by the RAG pipeline based on the provided query.
        """
        rag_chain = (
            {"context": self.retriever(), "question": RunnablePassthrough()}
            | self.prompt
            | self.llm 
            | StrOutputParser()
        )
        return rag_chain.invoke(query)


# For local test:
if __name__ == "__main__":

    llm = ChatGroq(
      model = "llama3-70b-8192", 
      temperature = 0.5, 
      api_key = "your_api_key"  
    ) 

    prompt = rag_system_prompt

    embedding = HuggingFaceEmbeddings(
        model_name = "sentence-transformers/all-mpnet-base-v2"
    )

    url = "https://en-m-wikipedia-org.translate.goog/wiki/Dark_wave?_x_tr_sl=en&_x_tr_tl=pt&_x_tr_hl=pt&_x_tr_pto=tc"

    rag_bot = RagBot(
        llm       = llm, 
        prompt    = prompt, 
        embedding = embedding, 
        documents = url
    )

    print("Ol√°! Eu sou o RagBot, digite a sua pergunta ou escreva 'sair' para encerrar a nossa conversa.")
    while True: 
        user_input = input("Mensagem: ")

        if user_input.lower() == "sair":
            print("Encerrando a conversa")
            break

        try: 
            response = rag_bot.run(user_input)
            print(f"Rag Bot: {response}")
        except Exception as e:
            print(f"Erro ao processar a resposta {e}")